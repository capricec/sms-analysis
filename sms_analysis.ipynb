{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn  # To improve the chart styling.\n",
    "import wordtree\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Javascript\n",
    "from wordcloud import STOPWORDS\n",
    "import ipywidgets as widgets\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#import iphone_connector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data from disk and set up the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total pages if all texts were printed: 24 (Arial size 12, single spaced)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "df1 = pd.read_csv(\"Sedus – Insights Content Template - All Data.csv\")\n",
    "df1['Content']=df1['Content'].fillna(\"\")\n",
    "#print(df1)\n",
    "#iphone_connector.initialize()\n",
    "\n",
    "#fully_merged_messages_df, address_book_df = iphone_connector.get_cleaned_fully_merged_messages()\n",
    "#full_names = set(address_book_df.full_name)  # Handy set to check for misspellings later on.\n",
    "#fully_merged_messages_df.full_name.replace('nan nan nan', 'Unknown', inplace=True)\n",
    "\n",
    "WORDS_PER_PAGE = 450  # Based upon http://wordstopages.com/\n",
    "print('\\nTotal pages if all texts were printed: {0:,d} (Arial size 12, single spaced)\\n'.format(\n",
    "    sum(df1.Content.apply(lambda x: len(x.split())))//WORDS_PER_PAGE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diving deeper into the actual text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a word tree of texts exchanged with a specific contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this requires an internet connection to load Google's JS library.\n",
    "def get_json_for_word_tree(contact):\n",
    "    df = fully_merged_messages_df[(fully_merged_messages_df.full_name == contact)]\n",
    "    print('Exchanged {0:,} texts with {1}'.format(df.shape[0], contact))\n",
    "    \n",
    "    array_for_json = [[text[1]] for text in df.text.iteritems()]\n",
    "    array_for_json.insert(0, [['Phrases']])\n",
    "    return json.dumps(array_for_json)\n",
    "    \n",
    "CONTACT_NAME = 'Mom'\n",
    "ROOT_WORD = 'feel'\n",
    "HTML(wordtree.get_word_tree_html(get_json_for_word_tree('Mom'),\n",
    "                                 ROOT_WORD.lower(),\n",
    "                                 lowercase=True,\n",
    "                                 tree_type='double'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and data munging for TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = copy.copy(string.punctuation)\n",
    "punctuation += u'“”‘’\\ufffc\\uff0c'  # Include some UTF-8 punctuation that occurred.\n",
    "punct_regex = re.compile(u'[{0}]'.format(punctuation))\n",
    "spaces_regex = re.compile(r'\\s{2,}')\n",
    "numbers_regex = re.compile(r'\\d+')\n",
    "\n",
    "def clean_text(input_str):\n",
    "    processed = input_str.lower()\n",
    "    processed = punct_regex.sub('', processed)\n",
    "    # Also try: processed = numbers_regex.sub('_NUMBER_', processed)\n",
    "    processed = numbers_regex.sub('', processed)\n",
    "    processed = spaces_regex.sub(' ', processed)\n",
    "    \n",
    "    return processed\n",
    "\n",
    "# The normal stopwords list contains words like \"i'll\" which is unprocessed.\n",
    "processed_stopwords = [clean_text(word) for word in STOPWORDS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TFIDF matrix for all contacts\n",
    "\n",
    "Note the methods below focus on texts received from these contacts, not texts you've sent to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_name = df1.groupby('Content_Num')['Content'].apply(lambda x: ' '.join(x)).to_frame()\n",
    "#grouped_by_name.head()\n",
    "#print(grouped_by_name.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF sparse matrix is 0.08097076416015625MB\n",
      "TFIDF matrix has shape: (142, 7980)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import tokenize\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = TfidfVectorizer(preprocessor=clean_text,\n",
    "                             tokenizer=tokenize.WordPunctTokenizer().tokenize,\n",
    "                             stop_words=processed_stopwords,\n",
    "                             ngram_range=(1, 2), max_df=.9, max_features=50000)\n",
    "tfidf_transformed_dataset = vectorizer.fit_transform(grouped_by_name.Content)\n",
    "word_list = pd.Series(vectorizer.get_feature_names())\n",
    "\n",
    "print('TFIDF sparse matrix is {0}MB'.format(tfidf_transformed_dataset.data.nbytes / 1024 / 1024))\n",
    "print('TFIDF matrix has shape: {0}'.format(tfidf_transformed_dataset.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods to leverage the TFIDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_word_summary_for_content(content_num, top_n=25):\n",
    "   # content = convert_unicode_to_str_if_needed(content)\n",
    "    content_num = int(content_num)\n",
    "    tfidf_record = _get_tfidf_record_for_content(content_num)\n",
    "    if tfidf_record is None:\n",
    "        print('\"{0}\" was not found.'.format(content_num))\n",
    "        return\n",
    "    sorted_indices = tfidf_record.argsort()[::-1]\n",
    "    return pd.DataFrame({'Word': word_list.iloc[sorted_indices[:top_n]]}).reset_index(drop=True)\n",
    "\n",
    "def get_word_summary_for_diffs(content_num, other_content_num, top_n=25):\n",
    "    #content = convert_unicode_to_str_if_needed(content)\n",
    "    content_num = int(content_num)\n",
    "    #other_content = convert_unicode_to_str_if_needed(other_content)\n",
    "    other_content_num = int(other_content_num)\n",
    "    \n",
    "    tfidf_record_content = _get_tfidf_record_for_content(content_num)\n",
    "    tfidf_record_other_content = _get_tfidf_record_for_content(other_content_num)\n",
    "    \n",
    "    if tfidf_record_content is None or tfidf_record_other_content is None:\n",
    "        # Print out the first contact not found.\n",
    "        content_not_found = content if tfidf_record_content is None else other_content\n",
    "        print('\"{0}\" was not found.'.format(content_not_found))\n",
    "        return\n",
    "    sorted_indices = (tfidf_record_content - tfidf_record_other_content).argsort()[::-1]\n",
    "    return pd.DataFrame({'Word': word_list.iloc[sorted_indices[:top_n]]}).reset_index(drop=True)\n",
    "\n",
    "# Returns the row in the TFIDF matrix for a given contact by name.\n",
    "def _get_tfidf_record_for_content(content_num):\n",
    "    if content_num not in grouped_by_name.index:\n",
    "        return None\n",
    "    row = np.argmax(grouped_by_name.index == content_num)\n",
    "    return tfidf_transformed_dataset.getrow(row).toarray().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words that identify a specific contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34af23b280d4ba9b97a7ef72566b9a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='1', description='Content_Num:', placeholder='Enter number'), IntSlider(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.get_word_summary_for_content(content_num, top_n=25)>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "widgets.interact(\n",
    "    get_word_summary_for_content,\n",
    "    content_num=widgets.Text(value=\"1\", description='Content_Num:', placeholder='Enter number'),\n",
    "    top_n=widgets.IntSlider(min=5, max=100, step=1, value=5, description='Max words to show:')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words that identify the difference between two contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042a6224c62746b68bc9451d26546b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='1', description='Content1:', placeholder='Enter number'), Text(value='2', de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.get_word_summary_for_diffs(content_num, other_content_num, top_n=25)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "widgets.interact(\n",
    "    get_word_summary_for_diffs,\n",
    "    content_num=widgets.Text(value=\"1\", description='Content1:', placeholder='Enter number'),\n",
    "    other_content_num=widgets.Text(value=\"2\",description='Content2:', placeholder='Enter 2nd number'),\n",
    "    top_n=widgets.IntSlider(description='Max words to show:', min=5, max=20, step=1, value=5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- Take top 10 words from each Content Piece and save in array in dataframe\n",
    "- Visualize connections between these words -- using categories of magazine, and section names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
